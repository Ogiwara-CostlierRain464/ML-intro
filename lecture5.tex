\section*{第五回　線形識別モデルの回答}

\subsection*{対数関数と指数関数}

\begin{dmath*}
	\ln \frac{ 
\mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1)  \bigg) p(C_1)
}{
\mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2)  \bigg) p(C_2)
} = -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1) +\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2) + \ln \frac{p(C_1)}{p(C_2)}
\end{dmath*}
となることを示す。$x = \ln (\mathrm{exp}(x)) = \mathrm{exp}(\ln x)$より、

\begin{align*}
&\ln \frac{ 
\mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1)  \bigg) p(C_1)
}{
\mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2)  \bigg) p(C_2)
} \\
&= \ln \bigg( \mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1)  \bigg) p(C_1) \bigg) \bigg( \mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2)  \bigg) p(C_2) \bigg)^{-1} \\
&=\ln \bigg( \mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1)  \bigg) p(C_1) \bigg) - \ln \bigg( \mathrm{exp}\bigg( -\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2)  \bigg) p(C_2) \bigg) \\
&=  -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1) + \ln p(C_1) + \frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2) - \ln p(C_2) \\
&= -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1) +\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2) + \ln \frac{p(C_1)}{p(C_2)}
\end{align*}


\subsection*{線形回帰モデルへの変形}
\begin{align*}
a= -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1) 
+\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2) + \ln \frac{p(C_1)}{p(C_2)} 
\end{align*}

の時、
\begin{equation*}
	\mathbf{w} = \mathbf{\Sigma}^{-1} (\bm{\mu}_1 - \bm{\mu}_2 )
\end{equation*}

\begin{equation*}
	w_0 = -\frac{1}{2}\bm{\mu}_1^T \mathbf{\Sigma}^{-1} \bm{\mu}_1 + \frac{1}{2} \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}
\end{equation*}

とおくと、

\begin{equation*}
	a = \mathbf{w}^T\mathbf{x} + w_0
\end{equation*}

と表記できることを示す。

\begin{dmath*}
a = -\frac{1}{2} (\mathbf{x} - \bm{\mu}_1)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_1) 
+\frac{1}{2} (\mathbf{x} - \bm{\mu}_2)^T \mathbf{\Sigma}^{-1}  (\mathbf{x} - \bm{\mu}_2) + \ln \frac{p(C_1)}{p(C_2)} \\
= -\frac{1}{2} (  \mathbf{x}^T  \mathbf{\Sigma}^{-1} \mathbf{x} - \mathbf{x}^T   \mathbf{\Sigma}^{-1}  \bm{\mu}_1   -  \bm{\mu}_1^T \mathbf{\Sigma}^{-1}  \mathbf{x}  +   \bm{\mu}_1^T \mathbf{\Sigma}^{-1}   \bm{\mu}_1  ) +   -\frac{1}{2} (   \mathbf{x}^T \mathbf{\Sigma}^{-1}  \mathbf{x} - \mathbf{x}^T  \mathbf{\Sigma}^{-1}   \bm{\mu}_2 -  \bm{\mu}_2^T \mathbf{\Sigma}^{-1}  \mathbf{x} + \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2  )+ \ln \frac{p(C_1)}{p(C_2)}
\end{dmath*}


ここで、$ \mathbf{x}^T   \mathbf{\Sigma}^{-1}  \bm{\mu}_1   $はスカラなので転置をかけても等しくなるので、$ (\mathbf{x}^T   \mathbf{\Sigma}^{-1}  \bm{\mu}_1 )^T = \bm{\mu}_1^T {\mathbf{\Sigma}^{-1}}^T  \mathbf{x} =  \bm{\mu}_1^T \mathbf{\Sigma}^{-1}  \mathbf{x} $となる。また、$(ABC)^T = C^T B^T A^T$であることと、$\Sigma$は対称行列で、対称行列の逆行列もまた対称行列となることから、${\mathbf{\Sigma}^{-1}}^T = \mathbf{\Sigma}^{-1}$を使った。これらを使い、

\begin{dmath*}
a = -\frac{1}{2} \mathbf{x}^T  \mathbf{\Sigma}^{-1} \mathbf{x} +   \bm{\mu}_1^T \mathbf{\Sigma}^{-1}  \mathbf{x} 	-\frac{1}{2} \bm{\mu}_1^T \mathbf{\Sigma}^{-1}   \bm{\mu}_1  +\frac{1}{2} \mathbf{x}^T  \mathbf{\Sigma}^{-1} \mathbf{x} - \bm{\mu}_2^T \mathbf{\Sigma}^{-1}  \mathbf{x} + \frac{1}{2}  \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2+ \ln \frac{p(C_1)}{p(C_2)} \\
=  \bm{\mu}_1^T \mathbf{\Sigma}^{-1}  \mathbf{x} 	-\frac{1}{2} \bm{\mu}_1^T \mathbf{\Sigma}^{-1}   \bm{\mu}_1 - \bm{\mu}_2^T \mathbf{\Sigma}^{-1}  \mathbf{x} + \frac{1}{2}  \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2+\ln \frac{p(C_1)}{p(C_2)}
\end{dmath*}

ここで、$ \mathbf{w}^T\mathbf{x} + w_0$を計算すると、
\begin{dmath*}
\mathbf{w}^T\mathbf{x} + w_0 = ( \bm{\mu}_1^T - \bm{\mu}_2^T ) \mathbf{\Sigma}^{-1} \mathbf{x}-\frac{1}{2}\bm{\mu}_1^T \mathbf{\Sigma}^{-1} \bm{\mu}_1 + \frac{1}{2} \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2 +\ln \frac{p(C_1)}{p(C_2)} \\
= \bm{\mu}_1^T \mathbf{\Sigma}^{-1}\mathbf{x} - \bm{\mu}_2^T \mathbf{\Sigma}^{-1}\mathbf{x}-\frac{1}{2}\bm{\mu}_1^T \mathbf{\Sigma}^{-1} \bm{\mu}_1 + \frac{1}{2} \bm{\mu}_2^T \mathbf{\Sigma}^{-1} \bm{\mu}_2 +\ln \frac{p(C_1)}{p(C_2)}  
\end{dmath*}

となるので、$a = \mathbf{w}^T\mathbf{x} + w_0$が示せた。


\subsection*{シグモイド関数の微分}

$\sigma(x) = \frac{1}{1 + \mathrm{exp}(-x)}$の時、$\frac{d \sigma(x)}{dx}= \sigma(x)(1-\sigma(x))$を示そう。

\begin{dmath*}
\frac{d \sigma(x)}{dx}= \frac{d (1+e^{-x})^{-1}}{dx} = - (1+e^{-x})^{-2} (-1) e^{-x} \quad(\because \frac{d e^{-x}}{dx}\hiderel{=}-e^{-x} ) \\
= 	(1+e^{-x})^{-2} e^{-x}
\end{dmath*}

ここで、右辺を計算すると、

\begin{dmath*}
\sigma(x)(1-\sigma(x)) = (1+e^{-x})^{-1} \frac{1+e^{-x}-1}{1+e^{-x}} \\
= (1+e^{-x})^{-2} e^{-x} 
\end{dmath*}

よって、$\frac{d \sigma(x)}{dx}= \sigma(x)(1-\sigma(x))$を示した。

\subsection*{交差エントロピー誤差関数の微分}

$\frac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \sum_{n=1}^N (y_n - t_n) \bm{\phi}_n$を示そう。

\begin{dmath*}
\frac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = -\sum_{n=1}^N \frac{\partial}{\partial \mathbf{w}}(  t_n \ln y_n + (1-t_n)\ln(1-y_n)  ) \\
= -\sum_{n=1}^N  (  t_n \frac{\partial}{\partial \mathbf{w}} \ln y_n + (1-t_n) 	\frac{\partial}{\partial \mathbf{w}} \ln (1 - y_n))
\end{dmath*}

まずは$\frac{\partial}{\partial \mathbf{w}} \ln y_n$を計算しよう。

\begin{dmath*}
\frac{\partial}{\partial \mathbf{w}} \ln y_n = \frac{1}{y_n} \frac{\partial}{\partial \mathbf{w}} y_n \quad (\because  \frac{d \ln x}{dx} \hiderel{=} \frac{1}{x}) \\
=  \frac{1}{ \sigma(\mathbf{w}^T \bm{\phi}_n)} 	\frac{\partial}{\partial \mathbf{w}} \sigma(\mathbf{w}^T \bm{\phi}_n) \\
= \frac{1}{ \sigma(\mathbf{w}^T \bm{\phi}_n)}  \sigma(\mathbf{w}^T \bm{\phi}_n)  (1 -  \sigma(\mathbf{w}^T \bm{\phi}_n)  ) 	\bm{\phi}_n \quad (\because \frac{d \sigma(x)}{dx} \hiderel{=} \sigma(x)(1-\sigma(x))) \\
= (1 - y_n) \bm{\phi}_n
\end{dmath*}

次に、$\frac{\partial}{\partial \mathbf{w}} \ln (1 - y_n)$を計算しよう。
\begin{dmath*}
\frac{\partial}{\partial \mathbf{w}} \ln (1 - y_n) = \frac{1}{1 - y_n} \frac{\partial}{\partial \mathbf{w}} (1 - y_n) \\
= \frac{1}{1 - y_n} \cdot - \frac{\partial}{\partial \mathbf{w}} y_n	\\
= \frac{1}{1 - y_n} \cdot - y_n (1 - y_n) \bm{\phi}_n \\
= - y_n \bm{\phi}_n 
\end{dmath*}

これにより、
\begin{dmath*}
\frac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = -\sum_{n=1}^N  (  t_n  (1-t_n)\bm{\phi}_n   + (1-t_n)(-y_n \bm{\phi}_n))\\
= -\sum_{n=1}^N (t_n - y_n) \bm{\phi}_n \\
= \sum_{n=1}^N (y_n - t_n) \bm{\phi}_n
\end{dmath*}

と示せた。

\subsection*{意地悪な問題}
$\frac{\partial E(\mathbf{w})}{\partial \mathbf{w}} =  \sum_{n=1}^N (y_n - t_n) \phi_n = 0$となるような$\mathbf{w}$を解析的に導出してみよう。

\begin{dmath*}
\sum_{n=1}^N t_n \phi_n = \sum_{n=1}^N y_n \phi_n \\
\sum_{n=1}^N t_n \phi_n \hiderel{=} \sum_{n=1}^N  \sigma(\mathbf{w}^T \bm{\phi}_n) \phi_n
\end{dmath*}

さて、線形回帰の時には、
\begin{equation}
	\sum_{n=1}^N t_n \bm{\phi}_n = \bigg( \sum_{n=1}^N \bm{\phi}_n \bm{\phi}_n^{\top} \bigg) \mathbf{w}
\end{equation}

となり、$\Phi = \begin{pmatrix} 
\bm{\phi}(\mathbf{x}_1) \\
\bm{\phi}(\mathbf{x}_2) \\
\cdots \\
\bm{\phi}(\mathbf{x}_N) \\
\end{pmatrix}$ とおくことによって、$\Phi^{\top} \mathbf{t}   = \Phi^{\top} \Phi  \mathbf{w}$と変形できた。しかし、今回は非線形の関数$\sigma$があるために$\mathbf{w}$を$\Sigma$の外側に移動できない！(非線形関数は行列では表現できないため。)


\subsection*{線形識別モデルのヘッセ行列}

$\nabla E (\mathbf{w} ) = \sum_{n=1}^N (y_n - t_n) \bm{\phi}_n $の時に$\mathbf{H} = \nabla \nabla E(\mathbf{w}) = \sum_{n=1}^N y_n (1 - y_n) \bm{\phi}_n \bm{\phi}_n^{\top}$となることを示そう。ただし、$\frac{\partial x \mathbf{a}}{\partial \mathbf{w}} = \mathbf{a} \big( \frac{\partial x}{\partial \mathbf{w}} \big)^T \quad (x \text{は} \mathbf{w} \text{の関数})$。

$y_n' = y_n(1-y_n)\bm{\phi}_n$より、$\mathbf{H} = \sum \frac{\partial  y_n \bm{\phi}_n}{\partial \mathbf{w}} = \sum \bm{\phi}_n (y_n(1 - y_n) {\bm{\phi}_n})^T = \sum  y_n(1 - y_n) {\bm{\phi}_n} \bm{\phi}_n^T$。


\subsection*{softmax関数の微分}
$ y_k(\bm{\phi})  =  p(C_k | \bm{\phi}) = \frac{\exp(a_{k})}{\sum_i \exp(a_{i}) } \quad (a_k = \mathbf{w}_k^T \bm{\phi}) $に対し、$\frac{\partial y_k}{\partial a_j} = y_k (I_{kj} - y_j)$を示そう。

$k = j$の時は、

\begin{equation*}
	\frac{\partial  y_k}{\partial a_k} = \frac{\exp(a_k) (\sum_i \exp(a_i)) - \exp (a_k) \exp(a_k) }{(\sum_i \exp(a_i))^2} = y_k - y_k^2 = y_k(1 - y_k)
\end{equation*}

$k \neq j$の時は、

\begin{equation*}
	\frac{\partial y_k}{\partial a_j} = -\frac{\exp(a_k) \exp(a_j) }{(\sum_i \exp(a_i))^2} =  -y_k y_j = y_k(-y_j)
\end{equation*}

あとは、上の二つの式をうまくまとめることがこの問題のゴールとなる。

さて、単位行列$I$の各要素について、次のことがわかっている：
\begin{equation*}
I_{kj} =  \left \{
\begin{array}{l}
1　(k＝jのとき) \\
0　(k≠jのとき)
\end{array}
\right.	
\end{equation*}

これを使うと、上の二つの式を$y_k (I_{kj} - y_j)$とまとめることができる。

よって、$\frac{\partial y_k}{\partial a_j} = y_k (I_{kj} - y_j)$。




追記：上記の演習問題の解説について、「講義中の解説と異なるのでは？」という意見をいただいたので、追加で説明を行います。モデルの式が講義資料では$\frac{\exp(a_{k})}{\sum_i \exp(a_{j}) }$となっているところが$\frac{\exp(a_{k})}{\sum_i \exp(a_{i}) }$となっていますが、これは、$\frac{\partial y_k}{\partial a_j}$の中の$j$と区別をつけやすくするためです。

$\frac{\exp(a_{k})}{\sum_i \exp(a_{j}) }$では、$j$は総和の式の中の一要素を表すための変数、$\frac{\partial y_k}{\partial a_j}$では、「$a_0$から$a_K$までのうち、任意の要素$a_j$で微分する」という意味で$j$を変数として使っていました。

しかしながら、この二つの記号が計算中には混同しやすい、という意見をいただき、 $\frac{\exp(a_{k})}{\sum_i \exp(a_{i}) }$と変更しました。


また、$ y_k (I_{kj} - y_j)$でなぜ$y_k(1 - y_k)$($k = j$のとき)と$y_k(-y_j)$($k \neq j$のとき)をまとめられるのか、についても詳しく説明します。

$ y_k (I_{kj} - y_j)$の式について、$k = j$の時は、$ y_k (I_{kk} - y_k) = y_k (1 - y_k)$となります。

また、$k \neq j$の時は、$ y_k (I_{kj} - y_j) = y_k (0 - y_j) = y_k (- y_j)$となります。

結果的に、$ y_k (I_{kj} - y_j)$はこの二つの式をうまくまとめることができました。

$y_k(1 - y_k)$と$y_k(-y_j)$という式から$I_{kj}$を使うことが導出できる、のではなく、$I_{kj}$を使うと結果的にうまく二つの式をまとめられるよね…というニュアンスとなります。







